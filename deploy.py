import urllib.request
import subprocess
import argparse
import os

content_dir = 'document/en/ps5'

def update_html_tag(appcache=True):
    path = content_dir + '/index.html'
    with open(path, 'r') as file:
        content = file.read()

    if appcache:
        html_tag = '<html manifest="cache.appcache">'
    else:
        html_tag = '<html>'

    print(f'Updating HTML tag in {path} to {html_tag}...')

    # remove line containing <html and replace with new tag
    start_index = content.find('<html')
    end_index = content.find('>', start_index) + 1
    content = content[:start_index] + html_tag + content[end_index:]

    with open(path, 'w') as file:
        file.write(content)
    print(f'HTML tag in {path} updated successfully.')


def add_version(version=None):
    path = content_dir + '/index.html'
    with open(path, 'r') as file:
        content = file.read()

    version_index = content.find('<b id="version">')

    if version_index != -1:
        closing_b_index = content.find('</b>', version_index)
        version_opening_tag_end_index = version_index + len('<b id="version">')

        if closing_b_index != -1:
            if version is None:
                version = get_commit_count()

            if version is None:
                print('Error couldnt get commit count.')
                return

            updated_content = content[:version_opening_tag_end_index] + \
                ' | v1.04 | v1.' + version + content[closing_b_index:]

            with open(path, 'w') as file:
                file.write(updated_content)
            print(f'File {path} updated successfully.')
        else:
            print('error couldnt find closing tag.')
    else:
        print('Error couldnt find version tag.')


def get_commit_count():
    print('Fetching commit count...')
    url = 'https://api.github.com/repos/idlesauce/PS5-Exploit-Host/commits?per_page=1&page=1'
    response = urllib.request.urlopen(url)
    headers = response.info()
    link_header = headers.get('Link', '')

    links_split = link_header.split('<')

    print(link_header)
    print(len(links_split))

    if not len(links_split) == 3:
        return None

    start_index = links_split[2].find('&page=')
    end_index = links_split[2].find('>', start_index)

    if start_index != -1 and end_index != -1:
        result = links_split[2][start_index + len('&page='):end_index]
        return result
    else:
        return None


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Update version in HTML file.')
    parser.add_argument('--version', type=str, help='Optional version to use instead of fetching from API')
    appcache_group = parser.add_mutually_exclusive_group()
    appcache_group.add_argument('--appcache', dest='appcache', action='store_true')
    appcache_group.add_argument('--no-appcache', dest='appcache', action='store_false')
    parser.set_defaults(appcache=True)
    args = parser.parse_args()

    print(args.version)
    print(args.appcache)

    update_html_tag(args.appcache)
    add_version(args.version)
    
    if args.appcache:
        subprocess.run(["python", "appcache_manifest_generator.py", "-cf"])
    else:
        if os.path.exists(content_dir + '/cache.appcache'):
            os.remove(content_dir + '/cache.appcache')
        